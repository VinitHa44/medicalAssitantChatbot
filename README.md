# Medical Assistant Chatbot - Backend

MVP Backend for medical domain chatbot with RAG, voice capabilities, and Digital Twin persona.

## ğŸ“‹ Technical Documentation

### High-Level Design (HLD)

#### System Overview
The Medical Assistant Chatbot is a RAG-based conversational AI system designed to provide reliable medical information using a Digital Twin persona (Dr. Asha). The system leverages multiple cloud services, databases, and AI models to deliver accurate, context-aware responses.

#### Core Components
1. **API Layer**: FastAPI-based REST endpoints for text and voice interactions
2. **RAG Pipeline**: Retrieval-Augmented Generation for context-aware responses
3. **Vector Search**: Pinecone for semantic similarity matching
4. **LLM Engine**: Meta Llama 3.3 70B (via Groq API) for response generation
5. **Caching Layer**: Redis for performance optimization
6. **Persistent Storage**: MongoDB for chat history and documents
7. **Voice Processing**: Whisper (STT) + Coqui TTS for voice interactions
8. **Digital Twin**: Dr. Asha persona for empathetic medical guidance

#### Technology Stack
- **Framework**: FastAPI (async Python web framework)
- **LLM**: Meta Llama 3.3 70B Versatile (via Groq API, Chain of Thought reasoning)
- **Vector DB**: Pinecone (cloud-native vector database)
- **Database**: MongoDB (document store for chat history)
- **Cache**: Redis (in-memory cache, TTL: 1 hour)
- **Voice STT**: OpenAI Whisper (speech-to-text)
- **Voice TTS**: Coqui TTS (text-to-speech)
- **Embeddings**: sentence-transformers/all-MiniLM-L6-v2 (384-dim vectors)

---

### Low-Level Design (LLD)

#### 1. API Routes Layer (`api/routes/`)

**chat.py - Text Chat Endpoint**
```
POST /api/chat/text
â”œâ”€ Input: ChatRequest (query, session_id)
â”œâ”€ Process:
â”‚  â”œâ”€ Check Redis cache (cache_manager.get)
â”‚  â”œâ”€ If HIT: Return cached response
â”‚  â”œâ”€ If MISS:
â”‚  â”‚  â”œâ”€ Execute RAG pipeline
â”‚  â”‚  â”œâ”€ Save to MongoDB (query_repo.save_query)
â”‚  â”‚  â””â”€ Cache result (cache_manager.set, TTL=3600s)
â””â”€ Output: ChatResponse (response, sources, confidence, cached)
```

**voice.py - Voice Chat Endpoint**
```
POST /api/chat/voice
â”œâ”€ Input: Audio file (multipart/form-data) + session_id
â”œâ”€ Process:
â”‚  â”œâ”€ Save audio to temp directory
â”‚  â”œâ”€ Transcribe audio â†’ text (Whisper)
â”‚  â”œâ”€ Check cache with transcribed query
â”‚  â”œâ”€ If MISS: Run RAG pipeline
â”‚  â”œâ”€ Generate TTS audio response
â”‚  â”œâ”€ Save to MongoDB
â”‚  â””â”€ Cache result
â””â”€ Output: ChatResponse + audio_url + transcribed_query
```

**health.py - Health Check**
```
GET /api/health
â”œâ”€ Check:
â”‚  â”œâ”€ Redis connection (ping)
â”‚  â”œâ”€ MongoDB connection (ping)
â”‚  â””â”€ Pinecone status
â””â”€ Output: Service status + uptime
```

#### 2. Core Layer (`core/`)

**database.py - Connection Managers**
```python
MongoDB:
  - AsyncIOMotorClient (motor library)
  - Collections: queries, documents
  - Methods: get_queries_collection(), get_documents_collection()

RedisCache:
  - Redis async client
  - decode_responses=True (JSON serialization)
  - Methods: connect(), close(), ping()
```

**cache.py - Cache Manager**
```python
CacheManager:
  - _generate_key(query, session_id):
      â†’ MD5 hash of "query:session_id"
      â†’ Prefix: "chat:{hash}"
  
  - get(query, session_id):
      â†’ Retrieve from Redis
      â†’ JSON deserialize
      â†’ Returns: {response, sources, confidence} or None
  
  - set(query, response, session_id):
      â†’ JSON serialize
      â†’ SETEX with TTL=3600 (1 hour)
      â†’ Returns: bool (success)
  
  - clear_session(session_id):
      â†’ Pattern match: "chat:*:{session_id}"
      â†’ Delete all matching keys
```

**persona.py - Dr. Asha Digital Twin**
```python
SYSTEM_PROMPT:
  - Role: Empathetic medical assistant
  - Constraints: No diagnosis, no prescriptions
  - Safety: Emergency detection, disclaimers
  - Style: Clear, compassionate, evidence-based

CHAIN_OF_THOUGHT_PROMPT:
  - Structured reasoning template
  - Evidence analysis
  - Confidence estimation
  - Emergency detection
```

#### 3. Services Layer (`services/`)

**rag.py - RAG Pipeline**
```python
RAGPipeline.query(user_query, top_k, topic_filter):
  1. Embed query (text_processor.create_embedding)
     â†’ 384-dim vector
  
  2. Vector search (vector_db.search)
     â†’ Pinecone similarity search
     â†’ Return top-K matches (default K=3)
  
  3. Generate response (llm_service.generate_response)
     â†’ Context chunks + query â†’ Llama 3
     â†’ Chain of Thought reasoning
  
  4. Extract sources (_extract_sources)
     â†’ Deduplicate URLs
     â†’ Return Source objects
  
  5. Return: {response, sources, confidence, emergency}
```

**llm.py - Llama 3 LLM Service**
```python
LLMService.generate_response(query, context_chunks, use_chain_of_thought):
  1. Format context from chunks
  2. Build prompt:
     - SYSTEM_PROMPT (Dr. Asha persona)
     - CHAIN_OF_THOUGHT_PROMPT (if enabled)
     - Context chunks
     - User query
  
  3. Call Groq API:
     - Model: llama-3.3-70b-versatile
     - Temperature: 0.7
     - Max tokens: 1024
  
  4. Parse response:
     - Extract reasoning, response, confidence
     - Detect emergency keywords
  
  5. Return: {response, confidence, emergency, reasoning}
```

**vector_store.py - Pinecone Integration**
```python
VectorStore:
  - Index: "medical-chatbot"
  - Dimension: 384
  - Metric: cosine similarity
  
  - search(query_embedding, top_k, topic_filter):
      â†’ Pinecone query with filters
      â†’ Returns: [{id, score, metadata}]
  
  - upsert(vectors, metadata):
      â†’ Batch insert with metadata
      â†’ Metadata: {text, source_url, title, topic}
```

**text_processor.py - Embedding & Chunking**
```python
TextProcessor:
  - Model: sentence-transformers/all-MiniLM-L6-v2
  
  - chunk_text(text, chunk_size=2000, overlap=200):
      â†’ Recursive character splitting
      â†’ Preserves paragraphs/sentences
      â†’ Returns: List[str]
  
  - create_embedding(text):
      â†’ Encode with sentence-transformer
      â†’ Returns: np.array (384-dim)
  
  - batch_embed(texts):
      â†’ Batch processing for efficiency
      â†’ Returns: List[np.array]
```

**scraper.py - Web Scraper**
```python
MedicalScraper:
  - Sources: WHO, Mayo Clinic, CDC
  - Libraries: BeautifulSoup4, requests
  
  - scrape_url(url):
      â†’ Extract article content
      â†’ Clean HTML tags
      â†’ Extract title, metadata
      â†’ Returns: {url, title, content}
  
  - scrape_all():
      â†’ Parallel scraping (ThreadPoolExecutor)
      â†’ Error handling & retries
      â†’ Returns: List[Document]
```

**voice.py - Voice Processing**
```python
VoiceService:
  - STT Model: Whisper (base)
  - TTS Model: Coqui TTS (tacotron2)
  
  - transcribe_audio(audio_path):
      â†’ Load audio file
      â†’ Whisper.transcribe()
      â†’ Returns: {text, language, confidence}
  
  - text_to_speech(text, session_id):
      â†’ Generate audio with TTS
      â†’ Save to static/audio/{session_id}.wav
      â†’ Returns: audio_url
```

#### 4. Repositories Layer (`repositories/`)

**mongo_repo.py - MongoDB Operations**
```python
QueryRepository:
  - save_query(session_id, user_query, response, sources, confidence, cached):
      â†’ Insert to queries collection
      â†’ Timestamp: UTC
      â†’ Returns: inserted_id
  
  - get_history(session_id, limit=50):
      â†’ Find by session_id
      â†’ Sort: timestamp DESC
      â†’ Returns: List[QueryDocument]
  
  - get_stats():
      â†’ Aggregation pipeline
      â†’ Returns: {total_queries, cache_hit_rate, avg_confidence}
```

#### 5. Models Layer (`models/`)

**schemas.py - Pydantic Models**
```python
ChatRequest:
  - query: str (max_length=500)
  - session_id: str

ChatResponse:
  - response: str
  - sources: List[Source]
  - confidence: float (0.0-1.0)
  - cached: bool
  - audio_url: Optional[str]
  - transcribed_query: Optional[str]

Source:
  - url: str
  - title: Optional[str]
  - relevance_score: float
```

**database_models.py - MongoDB Models**
```python
QueryDocument:
  - _id: ObjectId
  - session_id: str
  - user_query: str
  - response: str
  - sources: List[dict]
  - confidence: float
  - cached: bool
  - timestamp: datetime
  - emergency: bool
```

---

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLIENT LAYER                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Web Frontend    â”‚              â”‚  Voice Client    â”‚            â”‚
â”‚  â”‚  (React/Vite)    â”‚              â”‚  (Microphone)    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚           â”‚                                  â”‚                       â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ HTTP/REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API GATEWAY LAYER                           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                    â”‚   FastAPI Server    â”‚                          â”‚
â”‚                    â”‚   (main.py)         â”‚                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                               â”‚                                      â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚                     â”‚                     â”‚              â”‚
â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”        â”‚
â”‚    â”‚ /chat    â”‚        â”‚  /voice    â”‚      â”‚  /health   â”‚        â”‚
â”‚    â”‚  /text   â”‚        â”‚            â”‚      â”‚            â”‚        â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â”‚         BUSINESS LOGIC LAYER            â”‚               â”‚
â”‚         â”‚                    â”‚                    â”‚               â”‚
â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚    â”‚    Cache Manager (Redis)     â”‚         â”‚  Health   â”‚        â”‚
â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚         â”‚  Checks   â”‚        â”‚
â”‚    â”‚  â”‚ HIT â†’ Return Cached  â”‚    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                               â”‚
â”‚    â”‚             â”‚ MISS            â”‚                               â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                  â”‚                                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚         â”‚   RAG Pipeline  â”‚                                        â”‚
â”‚         â”‚                 â”‚                                        â”‚
â”‚         â”‚  1. Embed Query â”œâ”€â”€â”€â”€â–º Text Processor                   â”‚
â”‚         â”‚                 â”‚      (sentence-transformers)           â”‚
â”‚         â”‚  2. Vector      â”œâ”€â”€â”€â”€â–º Pinecone Vector DB               â”‚
â”‚         â”‚     Search      â”‚      (similarity search)               â”‚
â”‚         â”‚                 â”‚                                        â”‚
â”‚         â”‚  3. LLM Gen     â”œâ”€â”€â”€â”€â–º Llama 3.3 70B                    â”‚
â”‚         â”‚                 â”‚      (Chain of Thought)                â”‚
â”‚         â”‚  4. Persona     â”œâ”€â”€â”€â”€â–º Dr. Asha Formatter               â”‚
â”‚         â”‚                 â”‚                                        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                  â”‚                                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚         â”‚  Voice Service  â”‚                                        â”‚
â”‚         â”‚  (if voice req) â”‚                                        â”‚
â”‚         â”‚                 â”‚                                        â”‚
â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                                        â”‚
â”‚         â”‚  â”‚ Whisper  â”‚   â”‚  (STT)                                 â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                                        â”‚
â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                                        â”‚
â”‚         â”‚  â”‚Coqui TTS â”‚   â”‚  (TTS)                                 â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                                        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA PERSISTENCE LAYER                         â”‚
â”‚                              â”‚                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚   Redis      â”‚    â”‚  MongoDB   â”‚    â”‚   Pinecone      â”‚      â”‚
â”‚   â”‚  (Cache)     â”‚    â”‚ (History)  â”‚    â”‚ (Vector Index)  â”‚      â”‚
â”‚   â”‚              â”‚    â”‚            â”‚    â”‚                 â”‚      â”‚
â”‚   â”‚ â€¢ TTL: 1hr   â”‚    â”‚ â€¢ queries  â”‚    â”‚ â€¢ 384-dim       â”‚      â”‚
â”‚   â”‚ â€¢ JSON data  â”‚    â”‚ â€¢ documentsâ”‚    â”‚ â€¢ Cosine sim    â”‚      â”‚
â”‚   â”‚ â€¢ Session    â”‚    â”‚ â€¢ Session  â”‚    â”‚ â€¢ Metadata      â”‚      â”‚
â”‚   â”‚   based      â”‚    â”‚   history  â”‚    â”‚   filtering     â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

External Services:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Groq API         â”‚  â”‚    Pinecone      â”‚  â”‚  HuggingFace       â”‚
â”‚  (Llama 3)        â”‚  â”‚    Cloud API     â”‚  â”‚  (Embeddings)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Data Flow Diagram

#### **Text Chat Flow**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User    â”‚
â”‚  Query   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POST /api/chat/text                 â”‚
â”‚ {query, session_id}                 â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cache Check (Redis)                 â”‚
â”‚ Key: MD5(query:session_id)          â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€ HIT â”€â”€â”€â–º Return Cached â”€â”€â”€â”€â”
     â”‚             (50-100ms)         â”‚
     â”‚                                â”‚
     â””â”€â”€â”€ MISS                        â”‚
          â”‚                           â”‚
          â–¼                           â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
     â”‚ 1. Text Processor   â”‚         â”‚
     â”‚    Embed Query      â”‚         â”‚
     â”‚    â†’ 384-dim vector â”‚         â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
            â”‚                         â”‚
            â–¼                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
     â”‚ 2. Pinecone Search  â”‚         â”‚
     â”‚    Vector Similarityâ”‚         â”‚
     â”‚    â†’ Top-K=3 chunks â”‚         â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
            â”‚                         â”‚
            â–¼                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
     â”‚ 3. LLM Generation   â”‚         â”‚
     â”‚    Llama 3.3 70B    â”‚         â”‚
     â”‚    + Chain of       â”‚         â”‚
     â”‚      Thought        â”‚         â”‚
     â”‚    + Dr. Asha       â”‚         â”‚
     â”‚      Persona        â”‚         â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
            â”‚                         â”‚
            â–¼                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
     â”‚ 4. Format Response  â”‚         â”‚
     â”‚    + Sources        â”‚         â”‚
     â”‚    + Confidence     â”‚         â”‚
     â”‚    + Emergency flag â”‚         â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
            â”‚                         â”‚
            â–¼                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
     â”‚ 5. Save to MongoDB  â”‚         â”‚
     â”‚    queries collectionâ”‚        â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
            â”‚                         â”‚
            â–¼                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
     â”‚ 6. Cache Result     â”‚         â”‚
     â”‚    Redis SETEX      â”‚         â”‚
     â”‚    TTL = 3600s      â”‚         â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
            â”‚                         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                      â”‚
                                      â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ Return ChatResponse    â”‚
                         â”‚ {response, sources,    â”‚
                         â”‚  confidence, cached}   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Voice Chat Flow**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User    â”‚
â”‚  Audio   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POST /api/chat/voice             â”‚
â”‚ multipart: {audio, session_id}   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Save Audio to temp/           â”‚
â”‚    /temp/{uuid}_{filename}       â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Whisper STT                   â”‚
â”‚    Audio â†’ Text                  â”‚
â”‚    transcribed_query             â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Cache Check (Redis)           â”‚
â”‚    Key: MD5(transcribed:session) â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€ HIT â”€â”€â”€â–º Use Cached â”€â”€â”€â”€â”€â”€â”
     â”‚                               â”‚
     â””â”€â”€â”€ MISS                       â”‚
          â”‚                          â”‚
          â–¼                          â”‚
     [Same RAG Flow as Text]        â”‚
     (Embed â†’ Search â†’ LLM)         â”‚
          â”‚                          â”‚
          â–¼                          â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
     â”‚ Cache Result        â”‚        â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
            â”‚                        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                     â”‚
                                     â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ 4. TTS Engine    â”‚
                          â”‚    Text â†’ Audio  â”‚
                          â”‚    Save to       â”‚
                          â”‚    static/audio/ â”‚
                          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ 5. Save MongoDB  â”‚
                          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ Return Response  â”‚
                          â”‚ + audio_url      â”‚
                          â”‚ + transcribed    â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Knowledge Base Setup Flow**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ python setup_knowledge_base.py  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Web Scraping                 â”‚
â”‚    WHO, Mayo Clinic, CDC        â”‚
â”‚    â†’ Extract articles           â”‚
â”‚    â†’ Clean HTML                 â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Text Processing              â”‚
â”‚    â†’ Chunk text                 â”‚
â”‚      (size=2000, overlap=200)   â”‚
â”‚    â†’ Create embeddings          â”‚
â”‚      (384-dim vectors)          â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Pinecone Upload              â”‚
â”‚    â†’ Batch upsert vectors       â”‚
â”‚    â†’ Add metadata               â”‚
â”‚      {text, url, title, topic}  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. MongoDB Storage              â”‚
â”‚    â†’ Save documents             â”‚
â”‚    â†’ Save metadata              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Database Design

#### **MongoDB Schema**

**Collection: queries**
```javascript
{
  "_id": ObjectId,              // Auto-generated
  "session_id": String,         // User session identifier
  "user_query": String,         // Original user question
  "response": String,           // Generated response
  "sources": [                  // Reference sources
    {
      "url": String,
      "title": String,
      "relevance_score": Float
    }
  ],
  "confidence": Float,          // 0.0 - 1.0
  "cached": Boolean,            // Was response cached?
  "emergency": Boolean,         // Emergency detected?
  "timestamp": ISODate,         // UTC timestamp
  "voice_enabled": Boolean      // Was this a voice query?
}

// Indexes:
// - session_id (ascending)
// - timestamp (descending)
// - cached (ascending) for analytics
```

**Collection: documents**
```javascript
{
  "_id": ObjectId,
  "url": String,                // Source URL
  "title": String,              // Article title
  "content": String,            // Full article text
  "topic": String,              // diabetes, hypertension, etc.
  "chunks": [                   // Pre-processed chunks
    {
      "text": String,
      "chunk_id": String,
      "vector_id": String       // Pinecone ID reference
    }
  ],
  "metadata": {
    "scraped_at": ISODate,
    "word_count": Int,
    "last_updated": ISODate
  },
  "status": String              // "indexed", "pending", "error"
}

// Indexes:
// - url (unique)
// - topic (ascending)
// - status (ascending)
```

#### **Redis Schema**

**Cache Keys Pattern:**
```
chat:{md5_hash}

Example:
chat:a1b2c3d4e5f6... â†’ {
  "response": "Diabetes is...",
  "sources": [...],
  "confidence": 0.89
}

TTL: 3600 seconds (1 hour)
```

**Key Patterns:**
- `chat:{hash}` - Individual query cache
- Pattern matching for session clear: `chat:*:{session_id}`

**Data Structure:**
```json
{
  "response": "Diabetes is a chronic condition...",
  "sources": [
    {
      "url": "https://www.who.int/diabetes",
      "title": "Diabetes - WHO",
      "relevance_score": 0.92
    }
  ],
  "confidence": 0.89
}
```

#### **Pinecone Schema**

**Index Configuration:**
```yaml
Name: medical-chatbot
Dimension: 384
Metric: cosine
Pods: 1 (starter)
```

**Vector Record:**
```python
{
  "id": "doc_{uuid}_chunk_{n}",     # Unique chunk ID
  "values": [0.123, -0.456, ...],   # 384-dim embedding vector
  "metadata": {
    "text": "Diabetes symptoms include...",
    "source_url": "https://...",
    "title": "Diabetes Overview",
    "topic": "diabetes",
    "chunk_index": 0,
    "word_count": 350
  }
}
```

**Metadata Filtering:**
```python
# Filter by topic
filter = {"topic": {"$eq": "diabetes"}}

# Filter by source
filter = {"source_url": {"$eq": "https://www.who.int/..."}}
```

---

### Caching Strategy

#### **Cache Architecture**

**Technology:** Redis (in-memory key-value store)

**TTL (Time To Live):** 3600 seconds (1 hour)

**Key Generation:**
```python
def _generate_key(query: str, session_id: str) -> str:
    content = f"{query.lower().strip()}:{session_id}"
    hash_key = hashlib.md5(content.encode()).hexdigest()
    return f"chat:{hash_key}"
```

**Cache Flow:**
1. **Request arrives** â†’ Generate cache key
2. **Check Redis** â†’ `GET chat:{hash}`
3. **If HIT**: Return cached data (50-100ms)
4. **If MISS**: Execute RAG pipeline (2-5 seconds)
5. **Store result**: `SETEX chat:{hash} 3600 {json_data}`

**Performance Impact:**
- **Cache Hit Rate**: ~40-60% for common medical queries
- **Latency Reduction**: 20-50x faster
- **Cost Savings**: Reduces Llama 3 API calls by 40-60%

**Session-based Caching:**
- Same query + same session = cached
- Same query + different session = separate cache entries
- Reason: Personalized context may differ per session

**Cache Invalidation:**
```python
# Clear all cache for a session
await cache_manager.clear_session(session_id)

# Manual flush (admin)
redis-cli FLUSHDB
```

**Cache Statistics:**
- Monitor hit/miss rates via MongoDB analytics
- Track `cached: true/false` field in queries collection

---

## ğŸ—ï¸ System Architecture Summary

**Request Flow:**
```
User â†’ FastAPI â†’ Cache Check â†’ RAG Pipeline â†’ LLM â†’ Response
                     â†“              â†“
                   Redis         Pinecone
                                    â†“
                                 MongoDB
```

**Technology Integration:**
- **FastAPI**: Async request handling
- **Redis**: Sub-millisecond cache retrieval
- **Pinecone**: Scalable vector search
- **MongoDB**: Persistent chat history
- **Llama 3**: Open-source advanced language understanding
- **Whisper/TTS**: Natural voice interactions

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ chat.py          # Text chat endpoint
â”‚       â”œâ”€â”€ voice.py         # Voice chat endpoint
â”‚       â””â”€â”€ health.py        # Health check
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ database.py          # MongoDB & Redis clients
â”‚   â”œâ”€â”€ cache.py             # Cache manager
â”‚   â””â”€â”€ persona.py           # Dr. Asha Digital Twin
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ schemas.py           # Pydantic models
â”‚   â””â”€â”€ database_models.py   # MongoDB models
â”œâ”€â”€ repositories/
â”‚   â””â”€â”€ mongo_repo.py        # MongoDB operations
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ scraper.py           # Medical data scraper
â”‚   â”œâ”€â”€ text_processor.py    # Chunking & embeddings
â”‚   â”œâ”€â”€ vector_store.py      # Pinecone integration
â”‚   â”œâ”€â”€ llm.py               # Gemini LLM
â”‚   â”œâ”€â”€ rag.py               # RAG pipeline
â”‚   â””â”€â”€ voice.py             # Voice processing
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_knowledge_base.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_text_processor.py
â”‚   â”œâ”€â”€ test_persona.py
â”‚   â””â”€â”€ test_cache.py
â”œâ”€â”€ config.py
â”œâ”€â”€ main.py
â””â”€â”€ requirements.txt
```

## ğŸš€ Setup

### 1. Install Dependencies

```bash
cd backend
pip install -r requirements.txt
```

### 2. Configure Environment

Copy `.env.example` to `.env` in the **project root** directory:

```bash
# From project root
cp .env.example .env
```

Required:
- `LLAMA_API_KEY` - Get from Groq Console (https://console.groq.com)
- `LLAMA_MODEL` - Model name (default: llama-3.3-70b-versatile)
- `PINECONE_API_KEY` - Get from Pinecone
- `PINECONE_ENVIRONMENT` - Your Pinecone environment (e.g., us-east-1)

### 3. Start Services

**MongoDB** (via Docker):
```bash
docker run -d -p 27017:27017 --name mongodb mongo:latest
```

**Redis** (via Docker):
```bash
docker run -d -p 6379:6379 --name redis redis:latest
```

### 4. Setup Knowledge Base

Scrape medical data and index to Pinecone:

```bash
python scripts/setup_knowledge_base.py
```

This will:
- Scrape WHO & Mayo Clinic articles
- Chunk and embed text
- Upload to Pinecone
- Save to MongoDB

### 5. Run Server

```bash
python main.py
```

Or with uvicorn:
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

## ğŸ§ª Testing

```bash
# Run all tests
cd backend && pytest tests/ -v

# With coverage report
cd backend && pytest tests/ --cov=. --cov-report=html

# Run specific test file
cd backend && pytest tests/test_api.py -v
```

## ğŸ”‘ Key Features

### RAG Pipeline
1. Query embedding (sentence-transformers)
2. Vector search (Pinecone top-K)
3. Context retrieval
4. LLM generation (Llama 3 with Chain of Thought)
5. Response formatting (Dr. Asha persona)

### Digital Twin - Dr. Asha
- Empathetic medical assistant persona
- Safety constraints (no diagnosis/prescriptions)
- Emergency detection
- Source citations
- Confidence scoring

### Caching
- Redis cache with TTL
- Query hash-based keys
- Session-aware caching
- Reduces LLM costs & latency

### Voice Processing
- Whisper for accurate transcription
- TTS for natural responses
- Audio format conversion
- Temporary file cleanup

## ğŸ“Š Data Flow

```
User Query (Text/Voice)
    â†“
Cache Check (Redis)
    â†“
[MISS] â†’ RAG Pipeline
    â†“
    1. Embed query
    2. Vector search (Pinecone)
    3. Retrieve top-K chunks
    4. Generate response (Llama 3)
    5. Format with persona
    â†“
Save to MongoDB
    â†“
Cache result
    â†“
Return response (+ audio if voice)
```

## ğŸ”§ Development

Add new medical topics:
1. Add URLs to `services/scraper.py::MEDICAL_URLS`
2. Run `scripts/setup_knowledge_base.py`

Modify persona:
- Edit `core/persona.py::SYSTEM_PROMPT`
- Adjust `CHAIN_OF_THOUGHT_PROMPT`
